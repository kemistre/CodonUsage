{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the date\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/00577/codon_usage.csv.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://archive.ics.uci.edu/ml/machine-learning-databases/00577/codon_usage.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip codon_usage.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    \"\"\"\n",
    "    Reads in a csv file and returns a dataframe\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_name, low_memory=False)\n",
    "\n",
    "dataset = read_data('codon_usage.csv')\n",
    "df = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = read_data('codon_usage.csv')\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.dropna(inplace=True)\n",
    "\n",
    "dataset['Kingdom'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Kingdom'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Kingdom'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['organism_kingdom'] = dataset['Kingdom'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Kingdom'] = dataset['Kingdom'].map({   'arc': 0, 'bct': 0, \n",
    "                                                'phg': 1, 'plm': 0, 'vrl':0, \n",
    "                                                'pln': 1, 'inv': 1, \n",
    "                                                'vrt': 1, 'mam': 1,\n",
    "                                                'rod': 1, 'pri': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Kingdom'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(dataset[dataset['SpeciesID']==353569].index)\n",
    "dataset = dataset.drop(dataset[dataset['SpeciesID']==1238].index)\n",
    "\n",
    "# dataset[dataset['SpeciesID']==353569]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['UUU'] = dataset['UUU'].astype(float)\n",
    "dataset['UUC'] = dataset['UUC'].astype(float)\n",
    "# dataset = minmax_scale(dataset)\n",
    "cols = dataset.select_dtypes(np.number).columns\n",
    "\n",
    "num_columns_list = list(cols)\n",
    "# num_columns_list\n",
    "\n",
    "num_columns_list.remove('Kingdom')\n",
    "num_columns_list.remove('DNAtype')\n",
    "num_columns_list.remove('SpeciesID')\n",
    "num_columns_list.remove('Ncodons')\n",
    "# num_columns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_column in num_columns_list:\n",
    "    dataset[num_column] = minmax_scale(dataset[num_column])\n",
    "    # df['a'] = minmax_scale(df['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 5:-1].values.astype(float)\n",
    "y = dataset.iloc[:, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleVsMultiModel:\n",
    "  def __init__(self, X, y, laplace_smoothing_value, discrete, laplace, total_classes=2):\n",
    "    # update your Naive Bayes class functions to account for a discrete classification using what we have learned in the class. \n",
    "    self.X =X\n",
    "    self.y = y\n",
    "    self.laplace_smoothing_value = laplace_smoothing_value\n",
    "    self.discrete = discrete\n",
    "    self.laplace = laplace\n",
    "    self.total_classes = total_classes\n",
    "\n",
    "\n",
    "\n",
    "  def splitData(self):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=0.3, random_state=1)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "  def fitDistribution(self, data):\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    dist = norm(mean, std)\n",
    "    return dist\n",
    "\n",
    "\n",
    "\n",
    "  def laplaceSmoothing(self, class0, class1):\n",
    "    # print('type 0 ', type(class0))\n",
    "    # print('size 0 ', class0.size)\n",
    "    # print('type 1 ', type(class1))\n",
    "    # print('size 1 ', class1.size)\n",
    "    smooth0 = (class0 + self.laplace_smoothing_value) / ( (class0.size) + self.laplace_smoothing_value * self.n_features)\n",
    "    smooth1 = (class1 + self.laplace_smoothing_value) / ( (class1.size) + self.laplace_smoothing_value * self.n_features)\n",
    "\n",
    "    return smooth0, smooth1\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "  def evaluate(self, y, y_predicted):\n",
    "    '''\n",
    "\n",
    "      Takes original classes and predicted classes as input\n",
    "\n",
    "      Return the values of precision, recall and accuracy\n",
    "    \n",
    "    '''\n",
    "    y = ( y==1 )\n",
    "    y_predicted = ( y_predicted == 1 )\n",
    "\n",
    "    precision = (y&y_predicted).sum() / y_predicted.sum()\n",
    "    recall = (y&y_predicted).sum() / y.sum()\n",
    "    accuracy = (y==y_predicted).sum() / y.size\n",
    "\n",
    "\n",
    "    return precision, recall, accuracy\n",
    "\n",
    "\n",
    "\n",
    "  # def probability(self, X, dist1, dist2, prior):\n",
    "  #   return prior * ( dist1.pdf(X[0]) *  dist2.pdf(X[1]))\n",
    "\n",
    "\n",
    "  def probability(self, data):\n",
    "    py0 = 1\n",
    "    py1 = 1\n",
    "\n",
    "    if self.discrete:\n",
    "      for i in range(self.n_features):\n",
    "        # py0 *= self.features['X'+str(i)+str(data[i])+'0']\n",
    "        # py1 *= self.features['X'+str(i)+str(data[i])+'1']\n",
    "        py0 *= self.features['X'+str(i)+'00']\n",
    "        py1 *= self.features['X'+str(i)+'01']\n",
    "\n",
    "    else:\n",
    "      for i in range(self.n_features):\n",
    "        py0 *= self.features['X'+str(i)+'0'].pdf(data[i])\n",
    "        py1 *= self.features['X'+str(i)+'1'].pdf(data[i])\n",
    "\n",
    "    \n",
    "    return  py0 * self.prior_0, py1 * self.prior_1\n",
    "\n",
    "\n",
    "\n",
    "  def laplaceProbability(self, data):\n",
    "    py0 = 1\n",
    "    py1 = 1\n",
    "    if self.discrete:\n",
    "      for i in range(self.n_features):\n",
    "        smooth0 , smooth1 = self.laplaceSmoothing( self.features['X'+ str(i) + '0'], self.features['X'+str(i)+'1'])\n",
    "        py0 *= smooth0\n",
    "        py1 *= smooth1\n",
    "    \n",
    "    else:\n",
    "      for i in range(self.n_features):\n",
    "        pdf0 = self.features['X'+str(i)+'0'].pdf(data[i])\n",
    "        pdf1 = self.features['X'+str(i)+'1'].pdf(data[i])\n",
    "        smooth0 , smooth1 = self.laplaceSmoothing(pdf0, pdf1)\n",
    "        py0 *= smooth0\n",
    "        py1 *= smooth1\n",
    "\n",
    "    return  py0 * self.prior_0, py1 * self.prior_1                                                                             \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  def fit(self):\n",
    "\n",
    "    self.X_train, self.X_test, self.y_train, self.y_test = self.splitData()\n",
    "\n",
    "    X0_train = self.X_train[self.y_train == 0]\n",
    "    X1_train = self.X_train[self.y_train == 1]\n",
    "\n",
    "    self.prior_1 = len(X1_train) / len(self.X_train)\n",
    "    self.prior_0 = len(X0_train) / len(self.X_train)\n",
    "\n",
    "    self.n_features = self.X_train.shape[1]\n",
    "    self.features = {}\n",
    "    \n",
    "    if self.discrete:\n",
    "\n",
    "      print('Discrete Data')\n",
    "\n",
    "      for i in range(self.n_features):\n",
    "        self.features['X'+str(i)+'00'] = ( (X0_train[ X0_train[:,i]==0 ]).sum() ) / len(X0_train)\n",
    "        self.features['X'+str(i)+'01'] = ( (X1_train[ X1_train[:,i]==0 ]).sum() ) / len(X1_train)\n",
    "        self.features['X'+str(i)+'10'] = ( (X0_train[ X0_train[:,i]==1 ]).sum() ) / len(X0_train)\n",
    "        self.features['X'+str(i)+'11'] = ( (X1_train[ X1_train[:,i]==1 ]).sum() ) / len(X1_train)\n",
    "\n",
    "    \n",
    "    else:\n",
    "\n",
    "      print('Continuous Data')\n",
    "\n",
    "      for i in range(self.n_features):\n",
    "        self.features['X'+str(i)+'0'] = self.fitDistribution(X0_train[:, i])\n",
    "        self.features['X'+str(i)+'1'] = self.fitDistribution(X1_train[:, i])\n",
    "\n",
    "\n",
    "    # self.X00_dist = self.fitDistribution(X0_train[:, 0])\n",
    "    # self.X01_dist = self.fitDistribution(X0_train[:, 1])\n",
    "\n",
    "    # self.X10_dist = self.fitDistribution(X1_train[:, 0])\n",
    "    # self.X11_dist = self.fitDistribution(X1_train[:, 1])\n",
    "\n",
    "  \n",
    "  \n",
    "  def predict(self):\n",
    "    count = 0\n",
    "    y_predicted = []\n",
    "    y = []\n",
    "    for sample, target in zip(self.X_test, self.y_test):\n",
    "      # py0 = self.probability(sample, self.X00_dist, self.X01_dist, self.prior_0)\n",
    "      # py1 = self.probability(sample, self.X10_dist, self.X11_dist, self.prior_1)\n",
    "\n",
    "      # print(\"P(y=0|%s = %.3f\" % (sample, py0*100))\n",
    "      # print(\"P(y=1|%s = %.3f\" % (sample, py1*100))\n",
    "\n",
    "      if self.laplace:\n",
    "        py0, py1 = self.laplaceProbability(sample)\n",
    "      \n",
    "      else:\n",
    "        py0, py1 = self.probability(sample)\n",
    "\n",
    "      # print(\"Model predicted class {} and truth was {}\".format(np.argmax([py0,py1]), target))\n",
    "      y_predicted.append(np.argmax([py0,py1]))\n",
    "      y.append(target)\n",
    "      # np.append(y_predicted, np.argmax([py0,py1]))\n",
    "      # np.append(y, target)\n",
    "      # y = target\n",
    "      if np.argmax([py0,py1]) != target :\n",
    "        count+=1\n",
    "\n",
    "      # precision, recall, accuracy = self.evaluate(y, y_predicted)\n",
    "      # print(\"precision:\", precision)\n",
    "      # print(\"recall:\", recall)\n",
    "      # print(\"accuracy:\", accuracy)\n",
    "\n",
    "    \n",
    "    print(count)\n",
    "    # print(y)\n",
    "    y_predicted = np.array(y_predicted)\n",
    "    y = np.array(y)\n",
    "\n",
    "    precision, recall, accuracy = self.evaluate(y, y_predicted)\n",
    "    print(\"precision:\", precision)\n",
    "    print(\"recall:\", recall)\n",
    "    print(\"accuracy:\", accuracy)\n",
    "\n",
    "    return y_predicted, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SingleVsMultiModel(X,y, laplace_smoothing_value=0.5, discrete=False, laplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted, y = clf.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_predicted)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20f3b51db45487e76e19b963685aeaff4d2f53ee3a54c0a9156da0ef575c3172"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
