{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the date\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/00577/codon_usage.csv.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://archive.ics.uci.edu/ml/machine-learning-databases/00577/codon_usage.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip codon_usage.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    \"\"\"\n",
    "    Reads in a csv file and returns a dataframe\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_name, low_memory=False)\n",
    "\n",
    "dataset = read_data('codon_usage.csv')\n",
    "df = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = read_data('codon_usage.csv')\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.dropna(inplace=True)\n",
    "\n",
    "dataset['Kingdom'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Kingdom'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Kingdom'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['organism_kingdom'] = dataset['Kingdom'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Kingdom'] = dataset['Kingdom'].map({   'arc': 0, 'bct': 0, \n",
    "                                                'phg': 1, 'plm': 0, 'vrl':0, \n",
    "                                                'pln': 1, 'inv': 1, \n",
    "                                                'vrt': 1, 'mam': 1,\n",
    "                                                'rod': 1, 'pri': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Kingdom'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(dataset[dataset['SpeciesID']==353569].index)\n",
    "dataset = dataset.drop(dataset[dataset['SpeciesID']==1238].index)\n",
    "\n",
    "# dataset[dataset['SpeciesID']==353569]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['UUU'] = dataset['UUU'].astype(float)\n",
    "dataset['UUC'] = dataset['UUC'].astype(float)\n",
    "# dataset = minmax_scale(dataset)\n",
    "cols = dataset.select_dtypes(np.number).columns\n",
    "\n",
    "num_columns_list = list(cols)\n",
    "# num_columns_list\n",
    "\n",
    "num_columns_list.remove('Kingdom')\n",
    "num_columns_list.remove('DNAtype')\n",
    "num_columns_list.remove('SpeciesID')\n",
    "num_columns_list.remove('Ncodons')\n",
    "# num_columns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_column in num_columns_list:\n",
    "    dataset[num_column] = minmax_scale(dataset[num_column])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 5:-1].values.astype(float)\n",
    "y = dataset.iloc[:, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "\n",
    "    def __init__(self, X, y, sampleSize=16):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.sampleSize = sampleSize\n",
    "        if self.sampleSize <= 0 and self.sampleSize > len(self.X):\n",
    "            return TypeError(\"sampleSize must be between 0 and \", len(X))\n",
    "\n",
    "\n",
    "\n",
    "    def split_data(self, test_size=0.3, random_state=1):\n",
    "        self.X_train, self.X_rem, self.y_train, self.y_rem = tts(self.X, self.y, test_size=test_size, random_state=random_state)\n",
    "        self.X_test, self.X_val, self.y_test, self.y_val = tts(self.X_rem, self.y_rem, test_size=0.5, random_state=random_state)\n",
    "        return self.X_train, self.X_test, self.y_train, self.y_test\n",
    "\n",
    "    \n",
    "    # def training_data_generator(self):\n",
    "    #     # print(len(self.X))\n",
    "    #     idx = np.random.randint(0, len(self.X_train), self.sampleSize)\n",
    "    #     if self.sampleSize <= 0 and self.sampleSize > len(self.X):\n",
    "    #         return TypeError(\"sampleSize must be between 0 and \", len(self.X))\n",
    "    #     train_X, train_y = self.X_train[idx], self.y_train[idx]\n",
    "    #     yield train_X\n",
    "    #     yield train_y\n",
    "    \n",
    "    # def validation_data_generator(self):\n",
    "    #     idx = np.random.randint(0, len(self.X_rem), self.sampleSize)\n",
    "    #     if self.sampleSize <= 0 and self.sampleSize > len(self.X):\n",
    "    #         return TypeError(\"sampleSize must be between 0 and \", len(self.X_rem))\n",
    "    #     test_X, test_y = self.X[idx], self.y[idx]\n",
    "    #     # return test_X, test_y\n",
    "    #     yield test_X\n",
    "    #     yield test_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = DataGenerator(y=y, X=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training, X_testing, y_training, y_testing = data_generator.split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleVsMultiModel:\n",
    "  def __init__(self, X_train, X_test, y_train, y_test, laplace_smoothing_value, laplace, total_classes=2):\n",
    "    self.laplace_smoothing_value = laplace_smoothing_value\n",
    "    self.laplace = laplace\n",
    "    self.total_classes = total_classes\n",
    "    self.X_train = X_train\n",
    "    self.X_test = X_test\n",
    "    self.y_train = y_train\n",
    "    self.y_test = y_test\n",
    "\n",
    "\n",
    "  def fitDistribution(self, data):\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    dist = norm(mean, std)\n",
    "    return dist\n",
    "\n",
    "\n",
    "  def laplaceSmoothing(self, class0, class1):\n",
    "    smooth0 = (class0 + self.laplace_smoothing_value) / ( (class0.size) + self.laplace_smoothing_value * self.n_features)\n",
    "    smooth1 = (class1 + self.laplace_smoothing_value) / ( (class1.size) + self.laplace_smoothing_value * self.n_features)\n",
    "\n",
    "    return smooth0, smooth1\n",
    "\n",
    "\n",
    "  def evaluate(self, y, y_predicted):\n",
    "    '''\n",
    "\n",
    "      Takes original classes and predicted classes as input\n",
    "\n",
    "      Return the values of precision, recall and accuracy\n",
    "    \n",
    "    '''\n",
    "    y = ( y==1 )\n",
    "    y_predicted = ( y_predicted == 1 )\n",
    "\n",
    "    precision = (y&y_predicted).sum() / y_predicted.sum()\n",
    "    recall = (y&y_predicted).sum() / y.sum()\n",
    "    accuracy = (y==y_predicted).sum() / y.size\n",
    "\n",
    "\n",
    "    return precision, recall, accuracy\n",
    "  \n",
    "\n",
    "  def probability(self, data):\n",
    "    py0 = 1\n",
    "    py1 = 1\n",
    "\n",
    "    for i in range(self.n_features):\n",
    "      py0 *= self.features['X'+str(i)+'0'].pdf(data[i])\n",
    "      py1 *= self.features['X'+str(i)+'1'].pdf(data[i])\n",
    "\n",
    "    \n",
    "    return  py0 * self.prior_0, py1 * self.prior_1\n",
    "\n",
    "\n",
    "  def laplaceProbability(self, data):\n",
    "    py0 = 1\n",
    "    py1 = 1\n",
    "\n",
    "    for i in range(self.n_features):\n",
    "      pdf0 = self.features['X'+str(i)+'0'].pdf(data[i])\n",
    "      pdf1 = self.features['X'+str(i)+'1'].pdf(data[i])\n",
    "      smooth0 , smooth1 = self.laplaceSmoothing(pdf0, pdf1)\n",
    "      py0 *= smooth0\n",
    "      py1 *= smooth1\n",
    "\n",
    "    return  py0 * self.prior_0, py1 * self.prior_1                                                                             \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  def fit(self):\n",
    "\n",
    "    X0_train = self.X_train[self.y_train == 0]\n",
    "    X1_train = self.X_train[self.y_train == 1]\n",
    "\n",
    "    self.prior_1 = len(X1_train) / len(self.X_train)\n",
    "    self.prior_0 = len(X0_train) / len(self.X_train)\n",
    "\n",
    "    self.n_features = self.X_train.shape[1]\n",
    "    self.features = {}\n",
    "    \n",
    "    for i in range(self.n_features):\n",
    "      self.features['X'+str(i)+'0'] = self.fitDistribution(X0_train[:, i])\n",
    "      self.features['X'+str(i)+'1'] = self.fitDistribution(X1_train[:, i])\n",
    "\n",
    "\n",
    "  \n",
    "  \n",
    "  def predict(self):\n",
    "    count = 0\n",
    "    y_predicted = []\n",
    "    y = []\n",
    "    for sample, target in zip(self.X_test, self.y_test):\n",
    "\n",
    "      if self.laplace:\n",
    "        py0, py1 = self.laplaceProbability(sample)\n",
    "      \n",
    "      else:\n",
    "        py0, py1 = self.probability(sample)\n",
    "\n",
    "      # print(\"Model predicted class {} and truth was {}\".format(np.argmax([py0,py1]), target))\n",
    "      y_predicted.append(np.argmax([py0,py1]))\n",
    "      y.append(target)\n",
    "      # np.append(y_predicted, np.argmax([py0,py1]))\n",
    "      # np.append(y, target)\n",
    "      # y = target\n",
    "      if np.argmax([py0,py1]) != target :\n",
    "        count+=1\n",
    "\n",
    "      # precision, recall, accuracy = self.evaluate(y, y_predicted)\n",
    "      # print(\"precision:\", precision)\n",
    "      # print(\"recall:\", recall)\n",
    "      # print(\"accuracy:\", accuracy)\n",
    "\n",
    "    \n",
    "    print(count)\n",
    "    # print(y)\n",
    "    y_predicted = np.array(y_predicted)\n",
    "    y = np.array(y)\n",
    "\n",
    "    precision, recall, accuracy = self.evaluate(y, y_predicted)\n",
    "    print(\"precision:\", precision)\n",
    "    print(\"recall:\", recall)\n",
    "    print(\"accuracy:\", accuracy)\n",
    "\n",
    "    return y_predicted, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SingleVsMultiModel(X_train= X_training, X_test=X_testing, y_train= y_training,y_test=y_testing, laplace_smoothing_value=0.5, laplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted, y = clf.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_predicted)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20f3b51db45487e76e19b963685aeaff4d2f53ee3a54c0a9156da0ef575c3172"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
